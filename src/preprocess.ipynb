{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import jieba\n",
    "import os\n",
    "from utils import DATA_PROCESSED_DIR, DATA_W2V_VECTOR_PATH, DATA_W2V_META_PATH, DATA_TEXT_SEQUENCES_PATH, DATA_LABELS_PATH\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from utils import CustomIterator\n",
    "import tqdm\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rate</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26825482</td>\n",
       "      <td>è«æŒ¨è„‘å­</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-25 20:24:25</td>\n",
       "      <td>çœ‹åˆ°å½±ç‰‡å¼€å¤´ åè°Šå…„å¼Ÿå‡ ä¸ªå­—çš„æ—¶å€™ æˆ‘å°±å¿ƒé‡Œæœ‰æ•°äº†â€¦â€¦æœç„¶â€¦..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26825482</td>\n",
       "      <td>å£¹å®‰Â²</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-03-26 12:35:51</td>\n",
       "      <td>ä¸€å®šè¦çœ‹IMAXä¸€å®šè¦çœ‹IMAXä¸€å®šè¦çœ‹IMAXï¼ç”»é¢å’Œç‰¹æ•ˆè¿œè¶…é¢„æœŸä»–Mçš„ï¼Œæœ€åè§†è§‰å¤§é«˜æ½®æˆ‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26825482</td>\n",
       "      <td>ZeonGin Sou</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-26 23:43:12</td>\n",
       "      <td>å¤§æ¦‚æ˜¯ä¸»åˆ›å›¢é˜Ÿè‡´åŠ›äºè®²ä¸€ä¸ªè‡ªå—¨è‡³æçš„é˜´è°‹è®ºï¼Œå¯¼è‡´å…¨ç‰‡çš„ç‰¹æ•ˆç«Ÿç„¶æ²¡æœ‰æœåŠ¡äºå‰§æƒ…å’Œåˆºæ¿€ï¼Œè¿™å¯¹äºä¸€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26825482</td>\n",
       "      <td>å¤å°æ™‚</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-03 17:43:26</td>\n",
       "      <td>å„˜ç®¡é™ä½äº†å¿ƒç†é æœŸï¼Œä½†æ˜¯é‚„æ˜¯æ²’æ–™åˆ°é€™ç½é›£ç‰‡èƒ½æ‹æˆå²è©©ç´šç½é›£ã€‚èƒ½æ„Ÿè¦ºå°æ¼”æ‡‰ç•¶æœ‰è‡´æ•¬é§­å®¢å¸åœ‹çš„æ„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26825482</td>\n",
       "      <td>äººæ°‘å—è·¯å£¹å·</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-13 12:22:24</td>\n",
       "      <td>åŠ ä¸Šæœ€åä¸€æ®µçš„ç¾éš¾ç‰¹æ•ˆç®—å®ƒå‹‰å¼ºåŠæ ¼å§ï¼Œåœ¨å¯¼æ¼”æœ¬äººçš„ä½œå“é‡Œåº”å½“æ’åœ¨å€’æ•°çš„ï¼Œå¾ˆå¤šé€»è¾‘æ¼æ´å’Œä¸€è¨€éš¾...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id      user_id  rate                 time  \\\n",
       "0  26825482         è«æŒ¨è„‘å­   1.0  2022-03-25 20:24:25   \n",
       "1  26825482          å£¹å®‰Â²   4.0  2022-03-26 12:35:51   \n",
       "2  26825482  ZeonGin Sou   1.0  2022-03-26 23:43:12   \n",
       "3  26825482          å¤å°æ™‚   2.0  2022-02-03 17:43:26   \n",
       "4  26825482       äººæ°‘å—è·¯å£¹å·   2.0  2022-02-13 12:22:24   \n",
       "\n",
       "                                             content  \n",
       "0                   çœ‹åˆ°å½±ç‰‡å¼€å¤´ åè°Šå…„å¼Ÿå‡ ä¸ªå­—çš„æ—¶å€™ æˆ‘å°±å¿ƒé‡Œæœ‰æ•°äº†â€¦â€¦æœç„¶â€¦..  \n",
       "1  ä¸€å®šè¦çœ‹IMAXä¸€å®šè¦çœ‹IMAXä¸€å®šè¦çœ‹IMAXï¼ç”»é¢å’Œç‰¹æ•ˆè¿œè¶…é¢„æœŸä»–Mçš„ï¼Œæœ€åè§†è§‰å¤§é«˜æ½®æˆ‘...  \n",
       "2  å¤§æ¦‚æ˜¯ä¸»åˆ›å›¢é˜Ÿè‡´åŠ›äºè®²ä¸€ä¸ªè‡ªå—¨è‡³æçš„é˜´è°‹è®ºï¼Œå¯¼è‡´å…¨ç‰‡çš„ç‰¹æ•ˆç«Ÿç„¶æ²¡æœ‰æœåŠ¡äºå‰§æƒ…å’Œåˆºæ¿€ï¼Œè¿™å¯¹äºä¸€...  \n",
       "3  å„˜ç®¡é™ä½äº†å¿ƒç†é æœŸï¼Œä½†æ˜¯é‚„æ˜¯æ²’æ–™åˆ°é€™ç½é›£ç‰‡èƒ½æ‹æˆå²è©©ç´šç½é›£ã€‚èƒ½æ„Ÿè¦ºå°æ¼”æ‡‰ç•¶æœ‰è‡´æ•¬é§­å®¢å¸åœ‹çš„æ„...  \n",
       "4  åŠ ä¸Šæœ€åä¸€æ®µçš„ç¾éš¾ç‰¹æ•ˆç®—å®ƒå‹‰å¼ºåŠæ ¼å§ï¼Œåœ¨å¯¼æ¼”æœ¬äººçš„ä½œå“é‡Œåº”å½“æ’åœ¨å€’æ•°çš„ï¼Œå¾ˆå¤šé€»è¾‘æ¼æ´å’Œä¸€è¨€éš¾...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = pd.read_csv('../data/preset/minibatch.csv')\n",
    "\n",
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rate</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26825482</td>\n",
       "      <td>è«æŒ¨è„‘å­</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-25 20:24:25</td>\n",
       "      <td>çœ‹åˆ°å½±ç‰‡å¼€å¤´ åè°Šå…„å¼Ÿå‡ ä¸ªå­—çš„æ—¶å€™ æˆ‘å°±å¿ƒé‡Œæœ‰æ•°äº†â€¦â€¦æœç„¶â€¦..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26825482</td>\n",
       "      <td>å£¹å®‰Â²</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-03-26 12:35:51</td>\n",
       "      <td>ä¸€å®šè¦çœ‹IMAXä¸€å®šè¦çœ‹IMAXä¸€å®šè¦çœ‹IMAXï¼ç”»é¢å’Œç‰¹æ•ˆè¿œè¶…é¢„æœŸä»–Mçš„ï¼Œæœ€åè§†è§‰å¤§é«˜æ½®æˆ‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>26825482</td>\n",
       "      <td>ZeonGin Sou</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-26 23:43:12</td>\n",
       "      <td>å¤§æ¦‚æ˜¯ä¸»åˆ›å›¢é˜Ÿè‡´åŠ›äºè®²ä¸€ä¸ªè‡ªå—¨è‡³æçš„é˜´è°‹è®ºï¼Œå¯¼è‡´å…¨ç‰‡çš„ç‰¹æ•ˆç«Ÿç„¶æ²¡æœ‰æœåŠ¡äºå‰§æƒ…å’Œåˆºæ¿€ï¼Œè¿™å¯¹äºä¸€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26825482</td>\n",
       "      <td>å¤å°æ™‚</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-03 17:43:26</td>\n",
       "      <td>å„˜ç®¡é™ä½äº†å¿ƒç†é æœŸï¼Œä½†æ˜¯é‚„æ˜¯æ²’æ–™åˆ°é€™ç½é›£ç‰‡èƒ½æ‹æˆå²è©©ç´šç½é›£ã€‚èƒ½æ„Ÿè¦ºå°æ¼”æ‡‰ç•¶æœ‰è‡´æ•¬é§­å®¢å¸åœ‹çš„æ„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26825482</td>\n",
       "      <td>äººæ°‘å—è·¯å£¹å·</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-13 12:22:24</td>\n",
       "      <td>åŠ ä¸Šæœ€åä¸€æ®µçš„ç¾éš¾ç‰¹æ•ˆç®—å®ƒå‹‰å¼ºåŠæ ¼å§ï¼Œåœ¨å¯¼æ¼”æœ¬äººçš„ä½œå“é‡Œåº”å½“æ’åœ¨å€’æ•°çš„ï¼Œå¾ˆå¤šé€»è¾‘æ¼æ´å’Œä¸€è¨€éš¾...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  movie_id      user_id  rate                 time  \\\n",
       "0      0  26825482         è«æŒ¨è„‘å­   1.0  2022-03-25 20:24:25   \n",
       "1      1  26825482          å£¹å®‰Â²   4.0  2022-03-26 12:35:51   \n",
       "2      2  26825482  ZeonGin Sou   1.0  2022-03-26 23:43:12   \n",
       "3      3  26825482          å¤å°æ™‚   2.0  2022-02-03 17:43:26   \n",
       "4      4  26825482       äººæ°‘å—è·¯å£¹å·   2.0  2022-02-13 12:22:24   \n",
       "\n",
       "                                             content  \n",
       "0                   çœ‹åˆ°å½±ç‰‡å¼€å¤´ åè°Šå…„å¼Ÿå‡ ä¸ªå­—çš„æ—¶å€™ æˆ‘å°±å¿ƒé‡Œæœ‰æ•°äº†â€¦â€¦æœç„¶â€¦..  \n",
       "1  ä¸€å®šè¦çœ‹IMAXä¸€å®šè¦çœ‹IMAXä¸€å®šè¦çœ‹IMAXï¼ç”»é¢å’Œç‰¹æ•ˆè¿œè¶…é¢„æœŸä»–Mçš„ï¼Œæœ€åè§†è§‰å¤§é«˜æ½®æˆ‘...  \n",
       "2  å¤§æ¦‚æ˜¯ä¸»åˆ›å›¢é˜Ÿè‡´åŠ›äºè®²ä¸€ä¸ªè‡ªå—¨è‡³æçš„é˜´è°‹è®ºï¼Œå¯¼è‡´å…¨ç‰‡çš„ç‰¹æ•ˆç«Ÿç„¶æ²¡æœ‰æœåŠ¡äºå‰§æƒ…å’Œåˆºæ¿€ï¼Œè¿™å¯¹äºä¸€...  \n",
       "3  å„˜ç®¡é™ä½äº†å¿ƒç†é æœŸï¼Œä½†æ˜¯é‚„æ˜¯æ²’æ–™åˆ°é€™ç½é›£ç‰‡èƒ½æ‹æˆå²è©©ç´šç½é›£ã€‚èƒ½æ„Ÿè¦ºå°æ¼”æ‡‰ç•¶æœ‰è‡´æ•¬é§­å®¢å¸åœ‹çš„æ„...  \n",
       "4  åŠ ä¸Šæœ€åä¸€æ®µçš„ç¾éš¾ç‰¹æ•ˆç®—å®ƒå‹‰å¼ºåŠæ ¼å§ï¼Œåœ¨å¯¼æ¼”æœ¬äººçš„ä½œå“é‡Œåº”å½“æ’åœ¨å€’æ•°çš„ï¼Œå¾ˆå¤šé€»è¾‘æ¼æ´å’Œä¸€è¨€éš¾...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_dataset.dropna(axis=0).reset_index()\n",
    "\n",
    "print(f'length: {len(dataset)}')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/user18302289/anaconda3/envs/news/lib/python3.7/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp06dzm6p8' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.754 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "def line2words(line: str) -> List[str]:\n",
    "    return list(jieba.cut(line))\n",
    "\n",
    "dataset['words_count'] = dataset['content'].map((lambda x: len(line2words(x))))\n",
    "\n",
    "EXPECT_SENTENSE_LENGTH = math.ceil(dataset['words_count'].quantile(0.75))\n",
    "print(EXPECT_SENTENSE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_LABELS_PATH, 'wb') as f:\n",
    "    pickle.dump(dataset['rate'].values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'çœ‹åˆ°': 1, 'å½±ç‰‡': 2, 'å¼€å¤´': 3, ' ': 4, 'åè°Š': 5, 'å…„å¼Ÿ': 6, 'å‡ ä¸ª': 7, 'å­—': 8, 'çš„': 9, 'æ—¶å€™': 10, 'æˆ‘': 11, 'å°±': 12, 'å¿ƒé‡Œæœ‰æ•°': 13, 'äº†': 14, 'â€¦': 15, 'æœç„¶': 16, '..': 17, 'ä¸€å®š': 18, 'è¦': 19, 'çœ‹': 20, 'IMAX': 21, 'ï¼': 22, 'ç”»é¢': 23, 'å’Œ': 24, 'ç‰¹æ•ˆ': 25, 'è¿œè¶…': 26, 'é¢„æœŸ': 27, 'ä»–': 28, 'M': 29, 'ï¼Œ': 30, 'æœ€å': 31, 'è§†è§‰': 32, 'å¤§': 33, 'é«˜æ½®': 34, 'æ•´ä¸ª': 35, 'å–Š': 36, 'æœ‰': 37, 'åé': 38, 'å§æ§½': 39, 'å§': 40, 'æœŸå¾…': 41, 'æ›´': 42, 'å¤š': 43, 'ç§‘å­¦': 44, 'å­¦è¯´': 45, 'æœª': 46, 'è¢«': 47, 'è¯å®': 48, 'ç†è®º': 49, 'æ¬': 50, 'ä¸Šé“¶å¹•': 51, 'éª‚': 52, 'äºº': 53, 'å¥¥æ–¯å¡': 54, 'é¢å¥–': 55, 'é©¬ä¸Š': 56, 'åœ¨å®¶': 57, 'çœ‹çœ‹': 58, 'èµ„æº': 59, 'ä¸å¥½': 60, 'å—': 61, 'ï¼Ÿ': 62, 'å¤§æ¦‚': 63, 'æ˜¯': 64, 'ä¸»åˆ›': 65, 'å›¢é˜Ÿ': 66, 'è‡´åŠ›äº': 67, 'è®²': 68, 'ä¸€ä¸ª': 69, 'è‡ª': 70, 'å—¨': 71, 'è‡³æ': 72, 'é˜´è°‹è®º': 73, 'å¯¼è‡´': 74, 'å…¨ç‰‡': 75, 'ç«Ÿç„¶': 76, 'æ²¡æœ‰': 77, 'æœåŠ¡': 78, 'äº': 79, 'å‰§æƒ…': 80, 'åˆºæ¿€': 81, 'è¿™': 82, 'å¯¹äº': 83, 'ä¸€éƒ¨': 84, 'ç¾éš¾ç‰‡': 85, 'æ¥è¯´': 86, 'å°±æ˜¯': 87, 'ä¸€åœº': 88, 'åˆ‡åˆ‡å®å®': 89, 'ç¾éš¾': 90, 'ã€‚': 91, 'ç›¸æ¯”': 92, 'èµ·': 93, 'ã€Š': 94, '2012': 95, 'ã€‹': 96, 'äººå®¶': 97, 'å¯æ˜¯': 98, 'è·Ÿ': 99, 'å„ç§': 100, 'å½¢å¼': 101, 'è¿½é€': 102, 'åˆ°': 103, 'æ‰': 104, 'ä¸æ˜¯': 105, 'å·æ‡’': 106, 'åœ°': 107, 'ç”Ÿé€ ': 108, 'ä¸€åª': 109, 'å¤–æ˜Ÿäºº': 110, 'æ': 111, 'å¼‚æ˜Ÿ': 112, 'ç¾å˜': 113, 'é‚£å¥—': 114, 'å‘¢': 115, 'PS': 116, 'ï¼š': 117, 'åšç‰©é¦†': 118, 'å½“ç„¶': 119, 'æ°¸è¿œ': 120, 'ç¥': 121, 'å•¦': 122, 'å„˜': 123, 'ç®¡': 124, 'é™ä½': 125, 'å¿ƒç†': 126, 'é æœŸ': 127, 'ä½†æ˜¯': 128, 'é‚„æ˜¯': 129, 'æ²’': 130, 'æ–™åˆ°': 131, 'é€™ç½': 132, 'é›£ç‰‡': 133, 'èƒ½': 134, 'æ‹': 135, 'æˆå²': 136, 'è©©ç´š': 137, 'ç½é›£': 138, 'èƒ½æ„Ÿ': 139, 'è¦ºå°': 140, 'æ¼”æ‡‰': 141, 'ç•¶æœ‰': 142, 'è‡´æ•¬': 143, 'é§­å®¢': 144, 'å¸åœ‹': 145, 'æ„æ€': 146, 'é™¤äº†': 147, 'ç´…è—å°è—¥': 148, 'ä¸¸': 149, 'æ¢—': 150, 'åè¦†': 151, 'å‡ºç¾': 152, 'é‚„æœ‰': 153, 'æœ€çµ‚': 154, 'äººé¡': 155, 'èˆ‡': 156, 'AI': 157, 'æˆ°çˆ­': 158, 'é»‘å¹•': 159, 'å°æ¼”': 160, 'åŠŸåº•': 161, 'åœ¨': 162, 'å¹¾å€‹': 163, 'æœˆçƒ': 164, 'éš•è½': 165, 'å¤§å ´é¢': 166, 'éƒ½': 167, 'å¥½çœ‹': 168, 'æ¥µäº†': 169, 'é›™ç·š': 170, 'å‰ªè¼¯': 171, 'å¾ˆ': 172, 'ä¹¾è„†': 173, 'æ²’æœ‰': 174, 'å•é¡Œ': 175, 'ä½†': 176, 'é€™åŠ‡': 177, 'æœ¬èª': 178, 'çœŸçš„': 179, 'å—': 180, 'å®Œå…¨': 181, 'ä¸é¡§': 182, 'é‚è¼¯': 183, 'æ˜å¤©': 184, 'ä¹‹': 185, 'å¾Œ': 186, '+': 187, 'æ˜Ÿéš›æ•ˆ': 188, 'æ‡‰': 189, 'ä¸€é€š': 190, 'å¤§é›œ': 191, 'ç‡´': 192, 'ç”Ÿæ¬ç¡¬å¥—': 193, 'ç½é›£ç‰‡': 194, 'çš®': 195, 'ç§‘å¹»ç‰‡': 196, 'æ ¸': 197, 'ç„¶å¾Œè‡ªä»¥': 198, 'ç‚º': 199, 'ç…½': 200, 'ç…½æƒ…': 201, 'å‰åŠæ®µ': 202, 'é‚„æŠ±': 203, 'è‘—': 204, 'ä¹Ÿ': 205, 'è¨±æ˜¯': 206, 'å€‹': 207, 'æ­£å¸¸': 208, 'æ€ªç¸': 209, 'ç‰‡å±•': 210, 'é–‹': 211, 'å¸Œæœ›': 212, 'åŠæ®µ': 213, 'ç›´æ¥': 214, 'å½±é™¢': 215, 'è£¡é ­': 216, 'ç—›': 217, 'å¤§é': 218, 'å¹´': 219, 'ä½•è‹¦': 220, 'åŠ ä¸Š': 221, 'ä¸€æ®µ': 222, 'ç®—': 223, 'å®ƒ': 224, 'å‹‰å¼º': 225, 'åŠæ ¼': 226, 'å¯¼æ¼”': 227, 'æœ¬äºº': 228, 'ä½œå“': 229, 'é‡Œ': 230, 'åº”å½“': 231, 'æ’': 232, 'å€’æ•°': 233, 'å¾ˆå¤š': 234, 'é€»è¾‘': 235, 'æ¼æ´': 236, 'ä¸€è¨€éš¾å°½': 237, 'åœ°æ–¹': 238, 'ç»è´¹': 239, 'ç‡ƒçƒ§': 240, '30': 241, 'åˆ†é’Ÿ': 242, 'æ²¡': 243, 'åŠæ³•': 244, 'å†': 245, 'ç…§é¡¾': 246, 'å…¶ä»–': 247, 'è¿˜æœ‰': 248, 'ä¸€çœ‹': 249, 'é‡‘ä¸»': 250, 'å¡è¿›å»': 251, 'æ–‡æ–‡': 252, 'ä¸': 253, 'éœ€è¦': 254, 'ä¼°è®¡': 255, 'è¿™æ‰': 256, 'ç”µå½±é™¢': 257, 'ç”µå½±': 258, 'æ— è§†': 259, 'å®šå¾‹': 260, 'å¤ª': 261, 'è¿‡äº': 262, 'ç‰›é€¼å‡': 263, 'ä¸€é¢—': 264, 'ğŸ˜‚': 265, 'ã€': 266, '4.0': 267, '=': 268, 'ã€‘': 269, 'å»ºè®®': 270, 'å†…å®£': 271, 'èµ¶ç´§': 272, 'æ”¶ç¼–': 273, 'è‰¾é»˜é‡Œ': 274, 'å¥‡': 275, 'ç­¹æ‹': 276, 'å¾·': 277, 'ç‰¹é‡Œ': 278, 'å…‹å ¡': 279, 'åˆ†å½¢': 280, 'ç²’å­': 281, 'ç”¨æ¥': 282, 'åš': 283, 'ç–«è‹—': 284, 'å†…': 285, 'çº³ç±³': 286, 'æœºå™¨äºº': 287, 'ç®€ç›´': 288, 'å®Œç¾': 289, 'ç½—å…°': 290, 'æ°´å¹³': 291, 'å‘æŒ¥': 292, 'é‚£ä¹ˆ': 293, 'å¤šå¹´': 294, 'é¢˜æ': 295, 'æ”¹å˜': 296, 'å¥—è·¯': 297, 'å´': 298, 'åŸºæœ¬ä¸€è‡´': 299, 'è¿™éƒ¨': 300, 'å€’': 301, 'ç®—æ˜¯': 302, 'å’–': 303, 'äº‘é›†': 304, 'å¦‚æœ': 305, 'æƒ³': 306, 'å»': 307, 'ç‰›': 308, 'x': 309, 'CG': 310, 'æŠ€æœ¯': 311, 'å¯ä»¥': 312, 'è¿˜è¡Œ': 313, 'ä¸ç®—': 314, 'ç‰¹åˆ«': 315, 'æ— è„‘': 316, 'åœºé¢': 317, 'ç¡®å®': 318, 'éå¸¸': 319, 'æƒŠè‰³': 320, 'è¶³å¤Ÿ': 321, 'ï½': 322, 'ç¼–å‰§': 323, 'å†™': 324, 'è¿™ç§': 325, 'ä¸œè¥¿': 326, 'æœ¬æ„': 327, 'ç²—åˆ¶æ»¥é€ ': 328, 'ç½‘å¤§': 329, 'ä¸ä»…': 330, 'æ—¶æ—¶åˆ»åˆ»': 331, 'ä¾®è¾±': 332, 'è§‚ä¼—': 333, 'æ™ºå•†': 334, 'è¿˜': 335, 'å¤–æ˜Ÿ': 336, 'æ™ºæ…§': 337, 'ç”Ÿå‘½': 338, 'å®Œ': 339, 'ä½ ': 340, 'çŸ¥é“': 341, 'ä»€ä¹ˆ': 342, 'å«': 343, 'ç‰©ç†': 344, 'ä½“è‚²è€å¸ˆ': 345, 'æ•™': 346, 'éš¾çœ‹': 347, 'ä¸€æ‰¹': 348, 'WTF': 349, 'é¬¼': 350, 'çƒ‚': 351, 'ä¸€å¡Œç³Šæ¶‚': 352, 'æ—¶é—´': 353, 'è¿‡å¾—': 354, 'çœŸå¿«': 355, 'ä¸æ•¢ç›¸ä¿¡': 356, 'å¥‡ä¸Š': 357, 'Midway': 358, 'å·²ç»': 359, 'ä¸¤å¹´': 360, 'å‰': 361, 'äº‹æƒ…': 362, 'è¿™æ¬¡': 363, 'ä¸‰å¹´': 364, 'ç£¨ä¸€å‰‘': 365, 'å›å½’': 366, 'ä¸‹æ¥': 367, 'è§‰å¾—': 368, 'ä¹±': 369, 'æ²¡æƒ³åˆ°': 370, '2022': 371, 'æ—¶æœŸ': 372, 'åŒç±»é¢˜æ': 373, 'åˆ›æ„': 374, 'æ–°é²œ': 375, 'ä¸€èˆ¬': 376, '1': 377, 'äº¿': 378, '5': 379, 'åƒä¸‡ç¾å…ƒ': 380, 'é¢„ç®—': 381, 'åˆ¶ä½œ': 382, 'å‡º': 383, 'æ•ˆæœ': 384, 'è€Œä¸”': 385, 'æŠ•èµ„': 386, 'ä¸ª': 387, 'æ——ä¸‹': 388, 'ä¸­å›½': 389, 'å¥³æ¼”å‘˜': 390, 'å¸¦èµ„': 391, 'è¿›ç»„': 392, 'æˆä»½': 393, 'æŒº': 394, 'æ— ': 395, 'è¯­': 396, 'è¿™ç‰‡': 397, 'æ€»ç»“': 398, 'ä¸€å¥': 399, 'è¯': 400, 'è‚¥å®…': 401, 'æ‹¯æ•‘': 402, 'å®‡å®™': 403, 'è„‘æ´': 404, 'å¤§å¼€': 405, 'ç¡¬å¡': 406, 'å…ƒç´ ': 407, 'è¿‡åˆ†': 408, 'è¿˜æ˜¯': 409, 'é€‚åˆ': 410, 'é“¶å¹•': 411, 'çˆ½ç‰‡': 412, 'æ–‡æˆ': 413, 'çˆ†': 414, 'è®¾å®š': 415, 'å¤ªè ¢': 416, 'æŠŠ': 417, 'é«˜åº¦æ–‡æ˜': 418, 'ç­': 419, 'è ¢': 420, 'åƒ': 421, 'è ¢è´§': 422, 'ä¸ç®¡': 423, 'å†…æ ¸': 424, 'åœç•™': 425, 'é™ˆæ—§': 426, 'ä½“ç³»': 427, 'æ–°æ„': 428, '2.5': 429, '/': 430, '10': 431, 'æ— è¶£': 432, 'æ‰¯': 433, 'å¤©é™…': 434, 'è¶…è¶Š': 435, 'ç§‘å¹»': 436, 'èŒƒç•´': 437, 'å¥‡å¹»': 438, 'å‰§æœ¬': 439, 'ç©ºå£³': 440, 'å¼': 441, 'è§’è‰²': 442, 'å……æ»¡': 443, 'ç€': 444, 'ä¸´æ—¶': 445, 'èµ·æ„': 446, 'ç½”é¡¾': 447, 'æ„å¤–': 448, 'è¿‡': 449, 'æ°”': 450, 'è·¯æ•°': 451, 'å…¨äººç±»': 452, 'å¾—': 453, 'æ–°å† ': 454, 'ä¹‹å': 455, 'å¤§è„‘': 456, 'ç¼©å°': 457, 'ä¸€åœˆ': 458, 'âœ˜': 459, '0': 460, 'è¿™è¾¹': 461, 'åˆå®¶æ¬¢': 462, 'å”¯ç‹¬': 463, 'ç”·ä¸»': 464, 'å‰å¦»': 465, 'ç°ä»»': 466, 'è€å…¬': 467, 'å¥³ä¸»': 468, 'å‰å¤«': 469, 'ç‰ºç‰²': 470, 'æœ‰ç‚¹': 471, 'æ— è¯­': 472, 'å’±ä»¬': 473, 'æµæµª': 474, 'åœ°çƒ': 475, 'å•¥': 476, 'åŒºåˆ«': 477, '(': 478, 'å¶ç„¶': 479, 'å‘ç°': 480, 'Charlie': 481, 'Plummer': 482, 'å‚æ¼”': 483, 'ä¸ç„¶': 484, 'è¿™ä¸ª': 485, 'åˆ†æ•°': 486, 'åŠé€€': 487}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "\n",
    "for words in CustomIterator(dataset['content'], line2words):\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 11, 12, 13, 14, 15, 15, 16, 15, 17]\n"
     ]
    }
   ],
   "source": [
    "def words2sequence(words: List[str], vocab: dict) -> List[int]:\n",
    "    return [vocab[word] for word in words]\n",
    "\n",
    "example_sequence = words2sequence(line2words(dataset.iloc[0]['content']), vocab)\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2): ( , å½±ç‰‡)\n",
      "(15, 15): (â€¦, â€¦)\n",
      "(10, 11): (æ—¶å€™, æˆ‘)\n",
      "(5, 2): (åè°Š, å½±ç‰‡)\n",
      "(15, 15): (â€¦, â€¦)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 44   8 147  19], shape=(4,), dtype=int64)\n",
      "['ç§‘å­¦', 'å­—', 'é™¤äº†', 'è¦']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 20:45:29.382876: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-06 20:45:31.719096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5459 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:18:00.0, compute capability: 8.6\n",
      "2022-04-06 20:45:31.720178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 8550 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:3b:00.0, compute capability: 8.6\n",
      "2022-04-06 20:45:31.720918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 650 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:86:00.0, compute capability: 8.6\n",
      "2022-04-06 20:45:31.721714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 6556 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:af:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATASET_PATH = os.path.join(DATA_PROCESSED_DIR, 'text_ds.tsv')\n",
    "\n",
    "with open(DATA_TEXT_SEQUENCES_PATH, 'wb') as f:\n",
    "  pickle.dump([line2words(line) for line in CustomIterator(dataset['content'])], f)\n",
    "\n",
    "with open(TEXT_DATASET_PATH, 'w') as f:\n",
    "    for line in CustomIterator(dataset['content']):\n",
    "        print('\\t'.join(line2words(line)), file=f)\n",
    "\n",
    "text_ds = tf.data.TextLineDataset(TEXT_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "# def custom_standardization(input_data):\n",
    "#   return line2words(input_data)\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "# sequence_length = 10\n",
    "sequence_length = EXPECT_SENTENSE_LENGTH\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    # standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'ï¼Œ', 'çš„', 'äº†', 'ã€‚', 'åœ¨', 'æ˜¯', 'å°±', 'å’Œ', 'ï¼', 'çœ‹', 'ç‰¹æ•ˆ', 'æœ€å', 'â€¦', 'ï¼Ÿ', 'éƒ½', 'è¿˜', 'ç”µå½±', 'ç¾éš¾ç‰‡']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[ 60 306 309  22 410 402 327   3 264  35   8 300   4  14  14 243  14   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0] => ['çœ‹åˆ°', 'å½±ç‰‡', 'å¼€å¤´', 'åè°Š', 'å…„å¼Ÿ', 'å‡ ä¸ª', 'å­—', 'çš„', 'æ—¶å€™', 'æˆ‘', 'å°±', 'å¿ƒé‡Œæœ‰æ•°', 'äº†', 'â€¦', 'â€¦', 'æœç„¶', 'â€¦', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[ 41  29  11  44  41  29  11  44  41  29  11  44  10 203   9  12 126 103\n",
      " 428 473   3   2  13 155  40  98  35  34 357   4 253 381 377  21  10 250\n",
      " 256  74   3 189 326   9 249  57 149   3 209 280 452  10  13  14 100   3\n",
      " 431  14 332 104] => ['ä¸€å®š', 'è¦', 'çœ‹', 'imax', 'ä¸€å®š', 'è¦', 'çœ‹', 'imax', 'ä¸€å®š', 'è¦', 'çœ‹', 'imax', 'ï¼', 'ç”»é¢', 'å’Œ', 'ç‰¹æ•ˆ', 'è¿œè¶…', 'é¢„æœŸ', 'ä»–', 'm', 'çš„', 'ï¼Œ', 'æœ€å', 'è§†è§‰', 'å¤§', 'é«˜æ½®', 'æˆ‘', 'æ•´ä¸ª', 'å–Š', 'äº†', 'æœ‰', 'åé', 'å§æ§½', 'å§', 'ï¼', 'æœŸå¾…', 'æ›´', 'å¤š', 'çš„', 'ç§‘å­¦', 'å­¦è¯´', 'å’Œ', 'æœª', 'è¢«', 'è¯å®', 'çš„', 'ç†è®º', 'æ¬', 'ä¸Šé“¶å¹•', 'ï¼', 'æœ€å', 'â€¦', 'éª‚', 'çš„', 'äºº', 'â€¦', 'å¥¥æ–¯å¡', 'é¢å¥–']\n",
      "[ 39   7 438 353 165 150  26 167 355 166   3 113   2 317  82   3  12 185\n",
      "  64 251  86  23   9 394   2  28 319  90  19 244  38 461 397   3  20   5\n",
      " 198 144  43  93  42   2 430 367  55  77  70   3  20 125  78  13   2 289\n",
      " 447 414  75 206] => ['å¤§æ¦‚', 'æ˜¯', 'ä¸»åˆ›', 'å›¢é˜Ÿ', 'è‡´åŠ›äº', 'è®²', 'ä¸€ä¸ª', 'è‡ª', 'å—¨', 'è‡³æ', 'çš„', 'é˜´è°‹è®º', 'ï¼Œ', 'å¯¼è‡´', 'å…¨ç‰‡', 'çš„', 'ç‰¹æ•ˆ', 'ç«Ÿç„¶', 'æ²¡æœ‰', 'æœåŠ¡', 'äº', 'å‰§æƒ…', 'å’Œ', 'åˆºæ¿€', 'ï¼Œ', 'è¿™', 'å¯¹äº', 'ä¸€éƒ¨', 'ç¾éš¾ç‰‡', 'æ¥è¯´', 'å°±æ˜¯', 'ä¸€åœº', 'åˆ‡åˆ‡å®å®', 'çš„', 'ç¾éš¾', 'ã€‚', 'ç›¸æ¯”', 'èµ·', 'ã€Š', '2012', 'ã€‹', 'ï¼Œ', 'äººå®¶', 'å¯æ˜¯', 'è·Ÿ', 'å„ç§', 'å½¢å¼', 'çš„', 'ç¾éš¾', 'è¿½é€', 'åˆ°', 'æœ€å', 'ï¼Œ', 'æ‰', 'ä¸æ˜¯', 'å·æ‡’', 'åœ°', 'ç”Ÿé€ ']\n",
      "[412 181 111   4 301 105   2  24  51 236 273 122 106  31  68 291 151 229\n",
      "   5 169 157 231 202 164  47  72   3 294   2 110 178 439   3 241 372 401\n",
      "   2 120 255 429 163  92   3 290  96   5 316 385  51   6   2 312 254 109\n",
      "   3 343  16 329] => ['å„˜', 'ç®¡', 'é™ä½', 'äº†', 'å¿ƒç†', 'é æœŸ', 'ï¼Œ', 'ä½†æ˜¯', 'é‚„æ˜¯', 'æ²’', 'æ–™åˆ°', 'é€™ç½', 'é›£ç‰‡', 'èƒ½', 'æ‹', 'æˆå²', 'è©©ç´š', 'ç½é›£', 'ã€‚', 'èƒ½æ„Ÿ', 'è¦ºå°', 'æ¼”æ‡‰', 'ç•¶æœ‰', 'è‡´æ•¬', 'é§­å®¢', 'å¸åœ‹', 'çš„', 'æ„æ€', 'ï¼Œ', 'é™¤äº†', 'ç´…è—å°è—¥', 'ä¸¸', 'çš„', 'æ¢—', 'åè¦†', 'å‡ºç¾', 'ï¼Œ', 'é‚„æœ‰', 'æœ€çµ‚', 'äººé¡', 'èˆ‡', 'ai', 'çš„', 'æˆ°çˆ­', 'é»‘å¹•', 'ã€‚', 'å°æ¼”', 'åŠŸåº•', 'é‚„æ˜¯', 'åœ¨', 'ï¼Œ', 'å¹¾å€‹', 'æœˆçƒ', 'éš•è½', 'çš„', 'å¤§å ´é¢', 'éƒ½', 'å¥½çœ‹']\n",
      "[384  13 458   3  20  12 182 325 383 373  21   2   6 318 248   3 422  27\n",
      " 311 282   6 418   3   2 304  52 232   9 456   3 351   2  39  58  16 221\n",
      "   6   4  13 478 398   2  65 386 403 224  23   9  81  15  54  26  91  38\n",
      "  57 116  22 347] => ['åŠ ä¸Š', 'æœ€å', 'ä¸€æ®µ', 'çš„', 'ç¾éš¾', 'ç‰¹æ•ˆ', 'ç®—', 'å®ƒ', 'å‹‰å¼º', 'åŠæ ¼', 'å§', 'ï¼Œ', 'åœ¨', 'å¯¼æ¼”', 'æœ¬äºº', 'çš„', 'ä½œå“', 'é‡Œ', 'åº”å½“', 'æ’', 'åœ¨', 'å€’æ•°', 'çš„', 'ï¼Œ', 'å¾ˆå¤š', 'é€»è¾‘', 'æ¼æ´', 'å’Œ', 'ä¸€è¨€éš¾å°½', 'çš„', 'åœ°æ–¹', 'ï¼Œ', 'å¤§æ¦‚', 'ç»è´¹', 'éƒ½', 'ç‡ƒçƒ§', 'åœ¨', 'äº†', 'æœ€å', '30', 'åˆ†é’Ÿ', 'ï¼Œ', 'æ²¡', 'åŠæ³•', 'å†', 'ç…§é¡¾', 'å‰§æƒ…', 'å’Œ', 'å…¶ä»–', 'ï¼Ÿ', 'è¿˜æœ‰', 'ä¸€ä¸ª', 'ä¸€çœ‹', 'å°±æ˜¯', 'è¢«', 'é‡‘ä¸»', 'åè°Š', 'å¡è¿›å»']\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))\n",
    "\n",
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 229.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (192,)\n",
      "contexts.shape: (192, 5)\n",
      "labels.shape: (192, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6088 - accuracy: 0.2188\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6046 - accuracy: 0.3698\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6004 - accuracy: 0.5260\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5961 - accuracy: 0.6667\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5919 - accuracy: 0.7552\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5875 - accuracy: 0.8542\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5831 - accuracy: 0.9167\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5786 - accuracy: 0.9323\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.5741 - accuracy: 0.9427\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5694 - accuracy: 0.9583\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5646 - accuracy: 0.9583\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5596 - accuracy: 0.9635\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5546 - accuracy: 0.9740\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5493 - accuracy: 0.9740\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5439 - accuracy: 0.9792\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5382 - accuracy: 0.9844\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5324 - accuracy: 0.9896\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5264 - accuracy: 0.9896\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5201 - accuracy: 0.9896\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.5136 - accuracy: 0.9896\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5068 - accuracy: 0.9896\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4999 - accuracy: 0.9896\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4926 - accuracy: 0.9896\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4851 - accuracy: 0.9896\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4773 - accuracy: 0.9896\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4692 - accuracy: 0.9896\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4608 - accuracy: 0.9896\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4522 - accuracy: 0.9896\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4432 - accuracy: 0.9896\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4340 - accuracy: 0.9896\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4244 - accuracy: 0.9896\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4145 - accuracy: 0.9896\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4043 - accuracy: 0.9896\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3938 - accuracy: 0.9896\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3830 - accuracy: 0.9896\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3719 - accuracy: 0.9896\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3605 - accuracy: 0.9896\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3487 - accuracy: 0.9896\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3367 - accuracy: 0.9896\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3243 - accuracy: 0.9896\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3117 - accuracy: 0.9896\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2987 - accuracy: 0.9896\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2854 - accuracy: 0.9896\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2719 - accuracy: 0.9896\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2580 - accuracy: 0.9896\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2439 - accuracy: 0.9896\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2295 - accuracy: 0.9896\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2148 - accuracy: 0.9896\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1999 - accuracy: 0.9896\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1848 - accuracy: 0.9896\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1694 - accuracy: 0.9896\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1537 - accuracy: 0.9896\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1379 - accuracy: 0.9896\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1219 - accuracy: 0.9896\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1057 - accuracy: 0.9896\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0893 - accuracy: 0.9896\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0727 - accuracy: 0.9896\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0560 - accuracy: 0.9896\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0392 - accuracy: 0.9896\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0223 - accuracy: 0.9896\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0053 - accuracy: 0.9896\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9882 - accuracy: 0.9896\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9710 - accuracy: 0.9896\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9538 - accuracy: 0.9896\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9366 - accuracy: 0.9896\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9193 - accuracy: 0.9896\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9021 - accuracy: 0.9896\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8848 - accuracy: 0.9896\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8677 - accuracy: 0.9896\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8506 - accuracy: 0.9896\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8335 - accuracy: 0.9896\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8166 - accuracy: 0.9896\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7997 - accuracy: 0.9896\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7830 - accuracy: 0.9896\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7664 - accuracy: 0.9896\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7500 - accuracy: 0.9896\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7338 - accuracy: 0.9896\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7177 - accuracy: 0.9948\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7018 - accuracy: 0.9948\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6861 - accuracy: 0.9948\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6707 - accuracy: 0.9948\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6555 - accuracy: 0.9948\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6405 - accuracy: 0.9948\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6257 - accuracy: 0.9948\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6112 - accuracy: 0.9948\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5970 - accuracy: 0.9948\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5830 - accuracy: 0.9948\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5693 - accuracy: 0.9948\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5559 - accuracy: 0.9948\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5427 - accuracy: 0.9948\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5299 - accuracy: 0.9948\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5173 - accuracy: 0.9948\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5050 - accuracy: 0.9948\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4929 - accuracy: 0.9948\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4812 - accuracy: 0.9948\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4698 - accuracy: 0.9948\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4586 - accuracy: 0.9948\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4477 - accuracy: 0.9948\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4371 - accuracy: 0.9948\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4268 - accuracy: 0.9948\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4167 - accuracy: 0.9948\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4069 - accuracy: 0.9948\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3974 - accuracy: 0.9948\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3881 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3791 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3703 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3618 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3535 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3455 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3377 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3301 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3228 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3156 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3087 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3020 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2954 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2891 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2830 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2770 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2712 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2656 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2602 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2549 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2498 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2448 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2400 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2353 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2308 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2264 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2221 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2179 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2139 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2100 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2062 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2025 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1990 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1955 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1921 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1889 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1857 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1826 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1796 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1767 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1739 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1711 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1685 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1659 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1633 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1609 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1585 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1562 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1539 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1517 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1496 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1475 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1455 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1435 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1416 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1397 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1379 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1361 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1344 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1327 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1311 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1295 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1279 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1264 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1249 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1235 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1221 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1207 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1194 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1181 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1168 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1155 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1143 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1131 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1120 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1109 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1097 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1087 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1076 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1066 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1056 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1046 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1036 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1027 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1018 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1009 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1000 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0991 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0983 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0974 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0966 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0958 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0951 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0943 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0936 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0928 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0921 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15b8047cd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=200, callbacks=[tensorboard_callback])\n",
    "# word2vec.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"word2_vec\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " w2v_embedding (Embedding)   multiple                  524288    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  524288    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,048,576\n",
      "Trainable params: 1,048,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1766728), started 7:03:05 ago. (Use '!kill 1766728' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6668d559f653aee8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6668d559f653aee8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = open(DATA_W2V_VECTOR_PATH, 'w')\n",
    "mf = open(DATA_W2V_META_PATH, 'w')\n",
    "\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  vf.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  mf.write(word + \"\\n\")\n",
    "  \n",
    "mf.close()\n",
    "vf.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "798719e834ce2667b284169792ed9be744f500b1bbad4a58ecba241c6661f77e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('news')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
