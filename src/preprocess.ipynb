{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import jieba\n",
    "import os\n",
    "from utils import DATA_PROCESSED_DIR, DATA_W2V_VECTOR_PATH, DATA_W2V_META_PATH\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tqdm\n",
    "import math\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rate</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26825482</td>\n",
       "      <td>莫挨脑子</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-25 20:24:25</td>\n",
       "      <td>看到影片开头 华谊兄弟几个字的时候 我就心里有数了……果然…..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26825482</td>\n",
       "      <td>壹安²</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-03-26 12:35:51</td>\n",
       "      <td>一定要看IMAX一定要看IMAX一定要看IMAX！画面和特效远超预期他M的，最后视觉大高潮我...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26825482</td>\n",
       "      <td>ZeonGin Sou</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-26 23:43:12</td>\n",
       "      <td>大概是主创团队致力于讲一个自嗨至极的阴谋论，导致全片的特效竟然没有服务于剧情和刺激，这对于一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26825482</td>\n",
       "      <td>夏小時</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-03 17:43:26</td>\n",
       "      <td>儘管降低了心理預期，但是還是沒料到這災難片能拍成史詩級災難。能感覺導演應當有致敬駭客帝國的意...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26825482</td>\n",
       "      <td>人民南路壹号</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-13 12:22:24</td>\n",
       "      <td>加上最后一段的灾难特效算它勉强及格吧，在导演本人的作品里应当排在倒数的，很多逻辑漏洞和一言难...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id      user_id  rate                 time  \\\n",
       "0  26825482         莫挨脑子   1.0  2022-03-25 20:24:25   \n",
       "1  26825482          壹安²   4.0  2022-03-26 12:35:51   \n",
       "2  26825482  ZeonGin Sou   1.0  2022-03-26 23:43:12   \n",
       "3  26825482          夏小時   2.0  2022-02-03 17:43:26   \n",
       "4  26825482       人民南路壹号   2.0  2022-02-13 12:22:24   \n",
       "\n",
       "                                             content  \n",
       "0                   看到影片开头 华谊兄弟几个字的时候 我就心里有数了……果然…..  \n",
       "1  一定要看IMAX一定要看IMAX一定要看IMAX！画面和特效远超预期他M的，最后视觉大高潮我...  \n",
       "2  大概是主创团队致力于讲一个自嗨至极的阴谋论，导致全片的特效竟然没有服务于剧情和刺激，这对于一...  \n",
       "3  儘管降低了心理預期，但是還是沒料到這災難片能拍成史詩級災難。能感覺導演應當有致敬駭客帝國的意...  \n",
       "4  加上最后一段的灾难特效算它勉强及格吧，在导演本人的作品里应当排在倒数的，很多逻辑漏洞和一言难...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = pd.read_csv('../data/preset/minibatch.csv')\n",
    "\n",
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rate</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26825482</td>\n",
       "      <td>莫挨脑子</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-25 20:24:25</td>\n",
       "      <td>看到影片开头 华谊兄弟几个字的时候 我就心里有数了……果然…..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26825482</td>\n",
       "      <td>壹安²</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-03-26 12:35:51</td>\n",
       "      <td>一定要看IMAX一定要看IMAX一定要看IMAX！画面和特效远超预期他M的，最后视觉大高潮我...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>26825482</td>\n",
       "      <td>ZeonGin Sou</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-26 23:43:12</td>\n",
       "      <td>大概是主创团队致力于讲一个自嗨至极的阴谋论，导致全片的特效竟然没有服务于剧情和刺激，这对于一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26825482</td>\n",
       "      <td>夏小時</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-03 17:43:26</td>\n",
       "      <td>儘管降低了心理預期，但是還是沒料到這災難片能拍成史詩級災難。能感覺導演應當有致敬駭客帝國的意...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26825482</td>\n",
       "      <td>人民南路壹号</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-13 12:22:24</td>\n",
       "      <td>加上最后一段的灾难特效算它勉强及格吧，在导演本人的作品里应当排在倒数的，很多逻辑漏洞和一言难...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  movie_id      user_id  rate                 time  \\\n",
       "0      0  26825482         莫挨脑子   1.0  2022-03-25 20:24:25   \n",
       "1      1  26825482          壹安²   4.0  2022-03-26 12:35:51   \n",
       "2      2  26825482  ZeonGin Sou   1.0  2022-03-26 23:43:12   \n",
       "3      3  26825482          夏小時   2.0  2022-02-03 17:43:26   \n",
       "4      4  26825482       人民南路壹号   2.0  2022-02-13 12:22:24   \n",
       "\n",
       "                                             content  \n",
       "0                   看到影片开头 华谊兄弟几个字的时候 我就心里有数了……果然…..  \n",
       "1  一定要看IMAX一定要看IMAX一定要看IMAX！画面和特效远超预期他M的，最后视觉大高潮我...  \n",
       "2  大概是主创团队致力于讲一个自嗨至极的阴谋论，导致全片的特效竟然没有服务于剧情和刺激，这对于一...  \n",
       "3  儘管降低了心理預期，但是還是沒料到這災難片能拍成史詩級災難。能感覺導演應當有致敬駭客帝國的意...  \n",
       "4  加上最后一段的灾难特效算它勉强及格吧，在导演本人的作品里应当排在倒数的，很多逻辑漏洞和一言难...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_dataset.dropna(axis=0).reset_index()\n",
    "\n",
    "print(f'length: {len(dataset)}')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/user18302289/anaconda3/envs/news/lib/python3.7/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmphqwh5vn2' -> '/tmp/jieba.cache'\n",
      "Loading model cost 0.781 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "def line2words(line: str) -> List[str]:\n",
    "    return list(jieba.cut(line))\n",
    "\n",
    "dataset['words_count'] = dataset['content'].map((lambda x: len(line2words(x))))\n",
    "\n",
    "EXPECT_SENTENSE_LENGTH = math.ceil(dataset['words_count'].quantile(0.75))\n",
    "print(EXPECT_SENTENSE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rate</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>26825482</td>\n",
       "      <td>亵渎电影</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-03-30 17:04:17</td>\n",
       "      <td>编剧写这种东西，本意是在写一个粗制滥造的网大吧，不仅时时刻刻在侮辱观众的智商，还侮辱了外星智...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>26825482</td>\n",
       "      <td>Henrique Asano</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-03-25 22:06:30</td>\n",
       "      <td>脑洞大开，硬塞中国元素是很过分了，当然还是适合银幕看看的无脑爽片</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>26825482</td>\n",
       "      <td>胖球</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2022-03-29 23:14:15</td>\n",
       "      <td>这不还行吗？剧情不算特别无脑，场面确实非常惊艳，就足够了～</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>26825482</td>\n",
       "      <td>Keith Lee</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-03-25 21:17:34</td>\n",
       "      <td>时间过得真快，不敢相信看罗兰艾默里奇上一部《Midway》已经是两年前的事情了。这次罗兰艾默...</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26825482</td>\n",
       "      <td>夏小時</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-02-03 17:43:26</td>\n",
       "      <td>儘管降低了心理預期，但是還是沒料到這災難片能拍成史詩級災難。能感覺導演應當有致敬駭客帝國的意...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  movie_id         user_id  rate                 time  \\\n",
       "9       9  26825482            亵渎电影   1.0  2022-03-30 17:04:17   \n",
       "13     13  26825482  Henrique Asano   3.0  2022-03-25 22:06:30   \n",
       "8       8  26825482              胖球   4.0  2022-03-29 23:14:15   \n",
       "12     12  26825482       Keith Lee   3.0  2022-03-25 21:17:34   \n",
       "3       3  26825482             夏小時   2.0  2022-02-03 17:43:26   \n",
       "\n",
       "                                              content  words_count  \n",
       "9   编剧写这种东西，本意是在写一个粗制滥造的网大吧，不仅时时刻刻在侮辱观众的智商，还侮辱了外星智...           47  \n",
       "13                   脑洞大开，硬塞中国元素是很过分了，当然还是适合银幕看看的无脑爽片           19  \n",
       "8                       这不还行吗？剧情不算特别无脑，场面确实非常惊艳，就足够了～           19  \n",
       "12  时间过得真快，不敢相信看罗兰艾默里奇上一部《Midway》已经是两年前的事情了。这次罗兰艾默...          113  \n",
       "3   儘管降低了心理預期，但是還是沒料到這災難片能拍成史詩級災難。能感覺導演應當有致敬駭客帝國的意...          137  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = 0.25\n",
    "\n",
    "idx = list(range(len(dataset)))\n",
    "\n",
    "random.shuffle(idx)\n",
    "\n",
    "dataset_train_idx, dataset_val_idx = idx[int(val_size * len(idx)):], idx[:int(val_size * len(idx))]\n",
    "\n",
    "dataset_train, dataset_val = dataset.iloc[dataset_train_idx], dataset.iloc[dataset_val_idx]\n",
    "\n",
    "print(f'length: {len(dataset_train)}')\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomIterator:\n",
    "    def __init__(self, dataset, operator = lambda x: x):\n",
    "        self.dataset = dataset\n",
    "        self.operator = operator\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for line in self.dataset:\n",
    "            yield self.operator(line)\n",
    "    \n",
    "train_contents = CustomIterator(dataset['content'], line2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '看到': 1, '影片': 2, '开头': 3, ' ': 4, '华谊': 5, '兄弟': 6, '几个': 7, '字': 8, '的': 9, '时候': 10, '我': 11, '就': 12, '心里有数': 13, '了': 14, '…': 15, '果然': 16, '..': 17, '一定': 18, '要': 19, '看': 20, 'IMAX': 21, '！': 22, '画面': 23, '和': 24, '特效': 25, '远超': 26, '预期': 27, '他': 28, 'M': 29, '，': 30, '最后': 31, '视觉': 32, '大': 33, '高潮': 34, '整个': 35, '喊': 36, '有': 37, '十遍': 38, '卧槽': 39, '吧': 40, '期待': 41, '更': 42, '多': 43, '科学': 44, '学说': 45, '未': 46, '被': 47, '证实': 48, '理论': 49, '搬': 50, '上银幕': 51, '骂': 52, '人': 53, '奥斯卡': 54, '颁奖': 55, '马上': 56, '在家': 57, '看看': 58, '资源': 59, '不好': 60, '吗': 61, '？': 62, '大概': 63, '是': 64, '主创': 65, '团队': 66, '致力于': 67, '讲': 68, '一个': 69, '自': 70, '嗨': 71, '至极': 72, '阴谋论': 73, '导致': 74, '全片': 75, '竟然': 76, '没有': 77, '服务': 78, '于': 79, '剧情': 80, '刺激': 81, '这': 82, '对于': 83, '一部': 84, '灾难片': 85, '来说': 86, '就是': 87, '一场': 88, '切切实实': 89, '灾难': 90, '。': 91, '相比': 92, '起': 93, '《': 94, '2012': 95, '》': 96, '人家': 97, '可是': 98, '跟': 99, '各种': 100, '形式': 101, '追逐': 102, '到': 103, '才': 104, '不是': 105, '偷懒': 106, '地': 107, '生造': 108, '一只': 109, '外星人': 110, '搞': 111, '异星': 112, '灾变': 113, '那套': 114, '呢': 115, 'PS': 116, '：': 117, '博物馆': 118, '当然': 119, '永远': 120, '神': 121, '啦': 122, '儘': 123, '管': 124, '降低': 125, '心理': 126, '預期': 127, '但是': 128, '還是': 129, '沒': 130, '料到': 131, '這災': 132, '難片': 133, '能': 134, '拍': 135, '成史': 136, '詩級': 137, '災難': 138, '能感': 139, '覺導': 140, '演應': 141, '當有': 142, '致敬': 143, '駭客': 144, '帝國': 145, '意思': 146, '除了': 147, '紅藍小藥': 148, '丸': 149, '梗': 150, '反覆': 151, '出現': 152, '還有': 153, '最終': 154, '人類': 155, '與': 156, 'AI': 157, '戰爭': 158, '黑幕': 159, '導演': 160, '功底': 161, '在': 162, '幾個': 163, '月球': 164, '隕落': 165, '大場面': 166, '都': 167, '好看': 168, '極了': 169, '雙線': 170, '剪輯': 171, '很': 172, '乾脆': 173, '沒有': 174, '問題': 175, '但': 176, '這劇': 177, '本認': 178, '真的': 179, '嗎': 180, '完全': 181, '不顧': 182, '邏輯': 183, '明天': 184, '之': 185, '後': 186, '+': 187, '星際效': 188, '應': 189, '一通': 190, '大雜': 191, '燴': 192, '生搬硬套': 193, '災難片': 194, '皮': 195, '科幻片': 196, '核': 197, '然後自以': 198, '為': 199, '煽': 200, '煽情': 201, '前半段': 202, '還抱': 203, '著': 204, '也': 205, '許是': 206, '個': 207, '正常': 208, '怪獸': 209, '片展': 210, '開': 211, '希望': 212, '半段': 213, '直接': 214, '影院': 215, '裡頭': 216, '痛': 217, '大過': 218, '年': 219, '何苦': 220, '加上': 221, '一段': 222, '算': 223, '它': 224, '勉强': 225, '及格': 226, '导演': 227, '本人': 228, '作品': 229, '里': 230, '应当': 231, '排': 232, '倒数': 233, '很多': 234, '逻辑': 235, '漏洞': 236, '一言难尽': 237, '地方': 238, '经费': 239, '燃烧': 240, '30': 241, '分钟': 242, '没': 243, '办法': 244, '再': 245, '照顾': 246, '其他': 247, '还有': 248, '一看': 249, '金主': 250, '塞进去': 251, '文文': 252, '不': 253, '需要': 254, '估计': 255, '这才': 256, '电影院': 257, '电影': 258, '无视': 259, '定律': 260, '太': 261, '过于': 262, '牛逼减': 263, '一颗': 264, '😂': 265, '【': 266, '4.0': 267, '=': 268, '】': 269, '建议': 270, '内宣': 271, '赶紧': 272, '收编': 273, '艾默里': 274, '奇': 275, '筹拍': 276, '德': 277, '特里': 278, '克堡': 279, '分形': 280, '粒子': 281, '用来': 282, '做': 283, '疫苗': 284, '内': 285, '纳米': 286, '机器人': 287, '简直': 288, '完美': 289, '罗兰': 290, '水平': 291, '发挥': 292, '那么': 293, '多年': 294, '题材': 295, '改变': 296, '套路': 297, '却': 298, '基本一致': 299, '这部': 300, '倒': 301, '算是': 302, '咖': 303, '云集': 304, '如果': 305, '想': 306, '去': 307, '牛': 308, 'x': 309, 'CG': 310, '技术': 311, '可以': 312, '还行': 313, '不算': 314, '特别': 315, '无脑': 316, '场面': 317, '确实': 318, '非常': 319, '惊艳': 320, '足够': 321, '～': 322, '编剧': 323, '写': 324, '这种': 325, '东西': 326, '本意': 327, '粗制滥造': 328, '网大': 329, '不仅': 330, '时时刻刻': 331, '侮辱': 332, '观众': 333, '智商': 334, '还': 335, '外星': 336, '智慧': 337, '生命': 338, '完': 339, '你': 340, '知道': 341, '什么': 342, '叫': 343, '物理': 344, '体育老师': 345, '教': 346, '难看': 347, '一批': 348, 'WTF': 349, '鬼': 350, '烂': 351, '一塌糊涂': 352, '时间': 353, '过得': 354, '真快': 355, '不敢相信': 356, '奇上': 357, 'Midway': 358, '已经': 359, '两年': 360, '前': 361, '事情': 362, '这次': 363, '三年': 364, '磨一剑': 365, '回归': 366, '下来': 367, '觉得': 368, '乱': 369, '没想到': 370, '2022': 371, '时期': 372, '同类题材': 373, '创意': 374, '新鲜': 375, '一般': 376, '1': 377, '亿': 378, '5': 379, '千万美元': 380, '预算': 381, '制作': 382, '出': 383, '效果': 384, '而且': 385, '投资': 386, '个': 387, '旗下': 388, '中国': 389, '女演员': 390, '带资': 391, '进组': 392, '戏份': 393, '挺': 394, '无': 395, '语': 396, '这片': 397, '总结': 398, '一句': 399, '话': 400, '肥宅': 401, '拯救': 402, '宇宙': 403, '脑洞': 404, '大开': 405, '硬塞': 406, '元素': 407, '过分': 408, '还是': 409, '适合': 410, '银幕': 411, '爽片': 412, '文戏': 413, '爆': 414, '设定': 415, '太蠢': 416, '把': 417, '高度文明': 418, '灭': 419, '蠢': 420, '像': 421, '蠢货': 422, '不管': 423, '内核': 424, '停留': 425, '陈旧': 426, '体系': 427, '新意': 428, '2.5': 429, '/': 430, '10': 431, '无趣': 432, '扯': 433, '天际': 434, '超越': 435, '科幻': 436, '范畴': 437, '奇幻': 438, '剧本': 439, '空壳': 440, '式': 441, '角色': 442, '充满': 443, '着': 444, '临时': 445, '起意': 446, '罔顾': 447, '意外': 448, '过': 449, '气': 450, '路数': 451, '全人类': 452, '得': 453, '新冠': 454, '之后': 455, '大脑': 456, '缩小': 457, '一圈': 458, '✘': 459, '0': 460, '这边': 461, '合家欢': 462, '唯独': 463, '男主': 464, '前妻': 465, '现任': 466, '老公': 467, '女主': 468, '前夫': 469, '牺牲': 470, '有点': 471, '无语': 472, '咱们': 473, '流浪': 474, '地球': 475, '啥': 476, '区别': 477, '(': 478, '偶然': 479, '发现': 480, 'Charlie': 481, 'Plummer': 482, '参演': 483, '不然': 484, '这个': 485, '分数': 486, '劝退': 487}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0\n",
    "\n",
    "for words in CustomIterator(dataset['content'], line2words):\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 11, 12, 13, 14, 15, 15, 16, 15, 17]\n"
     ]
    }
   ],
   "source": [
    "def words2sequence(words: List[str], vocab: dict) -> List[int]:\n",
    "    return [vocab[word] for word in words]\n",
    "\n",
    "example_sequence = words2sequence(line2words(dataset.iloc[0]['content']), vocab)\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2): (华谊, 影片)\n",
      "(15, 14): (…, 了)\n",
      "(1, 4): (看到,  )\n",
      "(3, 5): (开头, 华谊)\n",
      "(7, 8): (几个, 字)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 44   8 147  19], shape=(4,), dtype=int64)\n",
      "['科学', '字', '除了', '要']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 20:01:59.345089: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-06 20:02:01.657506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5459 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:18:00.0, compute capability: 8.6\n",
      "2022-04-06 20:02:01.658582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 8550 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:3b:00.0, compute capability: 8.6\n",
      "2022-04-06 20:02:01.659327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 650 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:86:00.0, compute capability: 8.6\n",
      "2022-04-06 20:02:01.660312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9687 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:af:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATASET_PATH = os.path.join(DATA_PROCESSED_DIR, 'text_ds.txt')\n",
    "\n",
    "with open(TEXT_DATASET_PATH, 'w') as f:\n",
    "    for line in CustomIterator(dataset['content']):\n",
    "        print(' '.join(line2words(line)), file=f)\n",
    "\n",
    "text_ds = tf.data.TextLineDataset(TEXT_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "# def custom_standardization(input_data):\n",
    "#   return line2words(input_data)\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "# sequence_length = 10\n",
    "sequence_length = EXPECT_SENTENSE_LENGTH\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    # standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '，', '的', '了', '。', '在', '是', '就', '和', '！', '看', '特效', '最后', '…', '？', '都', '还', '电影', '灾难片']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "[ 60 306 309  22 410 402 327   3 264  35   8 300   4  14  14 243  14   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0] => ['看到', '影片', '开头', '华谊', '兄弟', '几个', '字', '的', '时候', '我', '就', '心里有数', '了', '…', '…', '果然', '…', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[ 41  29  11  44  41  29  11  44  41  29  11  44  10 203   9  12 126 103\n",
      " 428 473   3   2  13 155  40  98  35  34 357   4 253 381 377  21  10 250\n",
      " 256  74   3 189 326   9 249  57 149   3 209 280 452  10  13  14 100   3\n",
      " 431  14 332 104] => ['一定', '要', '看', 'imax', '一定', '要', '看', 'imax', '一定', '要', '看', 'imax', '！', '画面', '和', '特效', '远超', '预期', '他', 'm', '的', '，', '最后', '视觉', '大', '高潮', '我', '整个', '喊', '了', '有', '十遍', '卧槽', '吧', '！', '期待', '更', '多', '的', '科学', '学说', '和', '未', '被', '证实', '的', '理论', '搬', '上银幕', '！', '最后', '…', '骂', '的', '人', '…', '奥斯卡', '颁奖']\n",
      "[ 39   7 438 353 165 150  26 167 355 166   3 113   2 317  82   3  12 185\n",
      "  64 251  86  23   9 394   2  28 319  90  19 244  38 461 397   3  20   5\n",
      " 198 144  43  93  42   2 430 367  55  77  70   3  20 125  78  13   2 289\n",
      " 447 414  75 206] => ['大概', '是', '主创', '团队', '致力于', '讲', '一个', '自', '嗨', '至极', '的', '阴谋论', '，', '导致', '全片', '的', '特效', '竟然', '没有', '服务', '于', '剧情', '和', '刺激', '，', '这', '对于', '一部', '灾难片', '来说', '就是', '一场', '切切实实', '的', '灾难', '。', '相比', '起', '《', '2012', '》', '，', '人家', '可是', '跟', '各种', '形式', '的', '灾难', '追逐', '到', '最后', '，', '才', '不是', '偷懒', '地', '生造']\n",
      "[412 181 111   4 301 105   2  24  51 236 273 122 106  31  68 291 151 229\n",
      "   5 169 157 231 202 164  47  72   3 294   2 110 178 439   3 241 372 401\n",
      "   2 120 255 429 163  92   3 290  96   5 316 385  51   6   2 312 254 109\n",
      "   3 343  16 329] => ['儘', '管', '降低', '了', '心理', '預期', '，', '但是', '還是', '沒', '料到', '這災', '難片', '能', '拍', '成史', '詩級', '災難', '。', '能感', '覺導', '演應', '當有', '致敬', '駭客', '帝國', '的', '意思', '，', '除了', '紅藍小藥', '丸', '的', '梗', '反覆', '出現', '，', '還有', '最終', '人類', '與', 'ai', '的', '戰爭', '黑幕', '。', '導演', '功底', '還是', '在', '，', '幾個', '月球', '隕落', '的', '大場面', '都', '好看']\n",
      "[384  13 458   3  20  12 182 325 383 373  21   2   6 318 248   3 422  27\n",
      " 311 282   6 418   3   2 304  52 232   9 456   3 351   2  39  58  16 221\n",
      "   6   4  13 478 398   2  65 386 403 224  23   9  81  15  54  26  91  38\n",
      "  57 116  22 347] => ['加上', '最后', '一段', '的', '灾难', '特效', '算', '它', '勉强', '及格', '吧', '，', '在', '导演', '本人', '的', '作品', '里', '应当', '排', '在', '倒数', '的', '，', '很多', '逻辑', '漏洞', '和', '一言难尽', '的', '地方', '，', '大概', '经费', '都', '燃烧', '在', '了', '最后', '30', '分钟', '，', '没', '办法', '再', '照顾', '剧情', '和', '其他', '？', '还有', '一个', '一看', '就是', '被', '金主', '华谊', '塞进去']\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))\n",
    "\n",
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 234.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (184,)\n",
      "contexts.shape: (184, 5)\n",
      "labels.shape: (184, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练 word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6096 - accuracy: 0.2120\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6053 - accuracy: 0.4022\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6010 - accuracy: 0.5163\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5966 - accuracy: 0.6250\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5922 - accuracy: 0.8261\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5877 - accuracy: 0.8750\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5832 - accuracy: 0.9348\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5786 - accuracy: 0.9457\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5739 - accuracy: 0.9511\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5691 - accuracy: 0.9620\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5642 - accuracy: 0.9783\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5591 - accuracy: 0.9837\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5539 - accuracy: 0.9837\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5485 - accuracy: 0.9837\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5430 - accuracy: 0.9891\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5372 - accuracy: 0.9891\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5312 - accuracy: 0.9891\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5250 - accuracy: 0.9891\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5186 - accuracy: 0.9891\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5119 - accuracy: 0.9891\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5050 - accuracy: 0.9891\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4979 - accuracy: 0.9891\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4904 - accuracy: 0.9891\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4827 - accuracy: 0.9891\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4747 - accuracy: 0.9891\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4664 - accuracy: 0.9891\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4578 - accuracy: 0.9891\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4489 - accuracy: 0.9891\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4397 - accuracy: 0.9891\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4302 - accuracy: 0.9891\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4203 - accuracy: 0.9891\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4102 - accuracy: 0.9891\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3997 - accuracy: 0.9891\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3889 - accuracy: 0.9891\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3778 - accuracy: 0.9891\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3663 - accuracy: 0.9891\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3545 - accuracy: 0.9891\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3424 - accuracy: 0.9891\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3300 - accuracy: 0.9891\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3172 - accuracy: 0.9891\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3041 - accuracy: 0.9891\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2907 - accuracy: 0.9891\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2770 - accuracy: 0.9891\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2630 - accuracy: 0.9891\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2487 - accuracy: 0.9891\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2341 - accuracy: 0.9891\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2192 - accuracy: 0.9891\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2040 - accuracy: 0.9891\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1886 - accuracy: 0.9891\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1729 - accuracy: 0.9891\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1570 - accuracy: 0.9891\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1408 - accuracy: 0.9891\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1244 - accuracy: 0.9891\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1078 - accuracy: 0.9946\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0910 - accuracy: 0.9946\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0740 - accuracy: 0.9946\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0569 - accuracy: 0.9946\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0396 - accuracy: 0.9946\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0221 - accuracy: 0.9946\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0046 - accuracy: 0.9946\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9869 - accuracy: 0.9946\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9692 - accuracy: 0.9946\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9514 - accuracy: 0.9946\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9335 - accuracy: 0.9946\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9156 - accuracy: 0.9946\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8977 - accuracy: 0.9946\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8798 - accuracy: 0.9946\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8620 - accuracy: 0.9946\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8442 - accuracy: 0.9946\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8264 - accuracy: 0.9946\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8087 - accuracy: 0.9946\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7912 - accuracy: 0.9946\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7737 - accuracy: 0.9946\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7564 - accuracy: 0.9946\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7392 - accuracy: 0.9946\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7222 - accuracy: 0.9946\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7053 - accuracy: 0.9946\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6887 - accuracy: 0.9946\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6723 - accuracy: 0.9946\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6561 - accuracy: 0.9946\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6401 - accuracy: 0.9946\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6243 - accuracy: 0.9946\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6089 - accuracy: 0.9946\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5936 - accuracy: 0.9946\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5787 - accuracy: 0.9946\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5640 - accuracy: 0.9946\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5496 - accuracy: 0.9946\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5355 - accuracy: 0.9946\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5217 - accuracy: 0.9946\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5082 - accuracy: 0.9946\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4950 - accuracy: 0.9946\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4821 - accuracy: 0.9946\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4695 - accuracy: 0.9946\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4572 - accuracy: 0.9946\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4452 - accuracy: 0.9946\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4335 - accuracy: 0.9946\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4221 - accuracy: 0.9946\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4110 - accuracy: 0.9946\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4002 - accuracy: 0.9946\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3897 - accuracy: 0.9946\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3795 - accuracy: 0.9946\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3696 - accuracy: 0.9946\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3600 - accuracy: 0.9946\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3506 - accuracy: 0.9946\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3415 - accuracy: 0.9946\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3327 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3242 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3159 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3078 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3000 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2925 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2851 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2780 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2711 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2645 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2580 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2518 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2457 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2399 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2342 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2287 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2234 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2182 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2132 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2084 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2037 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1992 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1948 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1905 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1864 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1824 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1786 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1748 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1712 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1677 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1642 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1609 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1577 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1546 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1516 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1487 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1458 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1431 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1404 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1378 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1353 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1329 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1305 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1282 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1259 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1237 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1216 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1196 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1176 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1156 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1137 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1119 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1101 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1084 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1067 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1050 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1034 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1018 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1003 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0988 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0974 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0960 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0946 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0933 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0919 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0907 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0894 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0882 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0870 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0859 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0847 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0836 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0826 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0815 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0805 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0795 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0785 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0776 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0766 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0757 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0748 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0739 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0731 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0722 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0714 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0706 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0698 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0691 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0683 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0676 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0669 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0662 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0655 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0648 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0641 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f701c1022d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=200, callbacks=[tensorboard_callback])\n",
    "# word2vec.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"word2_vec\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " w2v_embedding (Embedding)   multiple                  524288    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  524288    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,048,576\n",
      "Trainable params: 1,048,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1766728), started 6:19:32 ago. (Use '!kill 1766728' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7cf309868803a284\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7cf309868803a284\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = open(DATA_W2V_VECTOR_PATH, 'w')\n",
    "mf = open(DATA_W2V_META_PATH, 'w')\n",
    "\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  vf.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  mf.write(word + \"\\n\")\n",
    "  \n",
    "mf.close()\n",
    "vf.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "798719e834ce2667b284169792ed9be744f500b1bbad4a58ecba241c6661f77e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('news')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
